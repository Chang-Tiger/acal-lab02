{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d547b164-71db-4e46-9838-487b8d2fd347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape inference complete ...\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import shape_inference\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "from onnx import onnx_ml_pb2 as xpb2\n",
    "\n",
    "onnx_model = onnx.load(\"mobilenetv2-10.onnx\", load_external_data=False)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "#print(onnx_model)\n",
    "\n",
    "inferred_model = shape_inference.infer_shapes(onnx_model)\n",
    "\n",
    "print('shape inference complete ...')\n",
    "\n",
    "def _parse_element_(elem: xpb2.ValueInfoProto):\n",
    "    name = getattr(elem, 'name', \"None\")\n",
    "    data_type = \"NA\"\n",
    "    shape_str = \"NA\"\n",
    "    etype = getattr(elem, 'type', False)\n",
    "    if etype:\n",
    "        ttype = getattr(etype, 'tensor_type', False)\n",
    "        if ttype:\n",
    "            data_type = getattr(ttype, 'elem_type', 0)\n",
    "            shape = getattr(elem.type.tensor_type, \"shape\", False)\n",
    "            if shape:\n",
    "                shape_str = \"[\"\n",
    "                dims = getattr(shape, 'dim', [])\n",
    "                for dim in dims:\n",
    "                    vals = getattr(dim, 'dim_value', \"?\")\n",
    "                    shape_str += (str(vals) + \",\")\n",
    "                shape_str = shape_str.rstrip(\",\")\n",
    "                shape_str += \"]\"\n",
    "    return name, data_type, shape_str\n",
    "def _parse_element(elem: xpb2.ValueInfoProto):\n",
    "    name = getattr(elem, 'name', \"None\")\n",
    "    shape_str = \"NA\"\n",
    "    etype = getattr(elem, 'type', False)\n",
    "    if etype:\n",
    "        ttype = getattr(etype, 'tensor_type', False)\n",
    "        if ttype:\n",
    "            shape = getattr(elem.type.tensor_type, \"shape\", False)\n",
    "            if shape:\n",
    "                shape_str = \"[\"\n",
    "                dims = getattr(shape, 'dim', [])\n",
    "                for dim in dims:\n",
    "                    vals = getattr(dim, 'dim_value', \"?\")\n",
    "                    shape_str += (str(vals) + \",\")\n",
    "                shape_str = shape_str.rstrip(\",\")\n",
    "                shape_str += \"]\"\n",
    "    return name, shape_str\n",
    "def find_operator(graph: xpb2.GraphProto):\n",
    "    try:\n",
    "        for i, node in enumerate(inferred_model.graph.node):\n",
    "                if node.name == \"\":\n",
    "                    inferred_model.graph.node[i].name = str(i)\n",
    "        if type(graph) is not xpb2.GraphProto:\n",
    "            sys.exit('The input graph is not a GraphProto!')\n",
    "\n",
    "        node_nList = [k.name for k in graph.node]\n",
    "        op_dict = {}\n",
    "        for node in graph.node:\n",
    "            if node.op_type in op_dict:\n",
    "                op_dict[node.op_type][node.name] = node_nList.index(node.name)\n",
    "            else:\n",
    "                op_dict[node.op_type] = {\n",
    "                    node.name: node_nList.index(node.name)\n",
    "                } \n",
    "\n",
    "        # init the list\n",
    "        list_of_layer = []\n",
    "        list_of_unknown_tensor = []\n",
    "\n",
    "        # get the occur #, total data element\n",
    "        for op_type in op_dict:\n",
    "            occur_num = len(op_dict[op_type])\n",
    "            total_data_elem = int(0)\n",
    "            unknown_tensor_list = []\n",
    "\n",
    "            for op in op_dict[op_type]:\n",
    "                op_data_elem = int(0)\n",
    "                for input_name in graph.node[op_dict[op_type][op]].input:\n",
    "                    input_nlist = [k.name for k in graph.input]\n",
    "                    initializer_nlist = [k.name for k in graph.initializer]\n",
    "                    value_info_nlist = [k.name for k in graph.value_info]\n",
    "                    output_nlist = [k.name for k in graph.output]\n",
    "\n",
    "                    # get tensor data\n",
    "                    if input_name in input_nlist:\n",
    "                        idx = input_nlist.index(input_name)\n",
    "                        proto= graph.input[idx]\n",
    "                        data_type = int(1)\n",
    "                    elif input_name in value_info_nlist:\n",
    "                        idx = value_info_nlist.index(input_name)\n",
    "                        proto= graph.value_info[idx]\n",
    "                        data_type = int(2)\n",
    "                    elif input_name in initializer_nlist:\n",
    "                        idx = initializer_nlist.index(input_name)\n",
    "                        proto= graph.initializer[idx]\n",
    "                        data_type = int(3)\n",
    "                    else:\n",
    "                        print(\"Can't find the tensor: \", input_name)\n",
    "                        print('input_nlist:\\n', input_nlist)\n",
    "                        print('===================')\n",
    "                        print('value_info_nlist:\\n', value_info_nlist)\n",
    "                        print('===================')\n",
    "                        print('initializer_nlist:\\n', initializer_nlist)\n",
    "                        print('===================')\n",
    "                        print('output_nlist:\\n', output_nlist)\n",
    "                        print('===================')\n",
    "                    if proto:\n",
    "                        input_tensor_size = int(1)\n",
    "                        if data_type == 1 or data_type == 2:\n",
    "                            name, shape_str = _parse_element(proto)\n",
    "                            \n",
    "                            if(len(shape_str)>2):#不是只有[]\n",
    "                                shape_str = shape_str.strip('[]')\n",
    "                                shape_str = shape_str.split(',')\n",
    "                                shape = []\n",
    "                                for dim in shape_str:\n",
    "                                    input_tensor_size *=int(dim)\n",
    "                        elif data_type == 3:\n",
    "                            # get the shape of the tensor\n",
    "                            shape = getattr(proto, 'dims', [])\n",
    "                            for dim in shape:\n",
    "                                input_tensor_size *= dim\n",
    "                        else:\n",
    "                            print(\n",
    "                                '[unexpected error] in [get_info] add_up_total_data_elem'\n",
    "                            )\n",
    "                    else:\n",
    "                        print(\"Can't find the input \", input_name, \" of the operator \",\n",
    "                            op, 'SKIP IT !')\n",
    "                        unknown_tensor_list.append(\n",
    "                            (op, input_name, graph.node[op_dict[op_type][op]].op_type))\n",
    "                        continue\n",
    "\n",
    "                    op_data_elem += input_tensor_size\n",
    "\n",
    "                total_data_elem += op_data_elem\n",
    "            list_of_layer.append((op_type, occur_num, total_data_elem))\n",
    "            list_of_unknown_tensor.extend(unknown_tensor_list)\n",
    "        print('list_of_layer BUILT ! ')\n",
    "\n",
    "        # resort the list\n",
    "        list_of_layer = sorted(list_of_layer,\n",
    "                                key=lambda Layer: Layer[2],\n",
    "                                reverse=True)\n",
    "        print('list_of_layer RESORTED ! ')\n",
    "\n",
    "        # Display result\n",
    "        columns = ['op_type', 'occur #', 'Total data elements #']\n",
    "        print(tabulate(list_of_layer, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "\n",
    "        columns = ['op_name', 'unfound_tensor', 'op_type']\n",
    "        print(tabulate(list_of_unknown_tensor, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "\n",
    "        print('DISPLAY successfully ! ')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unable to display: \" + str(e))\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "def get_attribute(graph: xpb2.GraphProto):\n",
    "    print(\"attribution\\n\")\n",
    "    try:\n",
    "        conv_attr = []\n",
    "        for i, node in enumerate(inferred_model.graph.node):\n",
    "                if node.name == \"\":\n",
    "                    inferred_model.graph.node[i].name = str(i)\n",
    "        # get the idx_list\n",
    "        # idx_list = get_op_type_name_and_idx(graph)\n",
    "        node_nlist = [k.name for k in graph.node]\n",
    "        idx_list = {}\n",
    "        for node in graph.node:\n",
    "            if node.op_type in idx_list:\n",
    "                idx_list[node.op_type][node.name] = node_nlist.index(node.name)\n",
    "            else:\n",
    "                idx_list[node.op_type] = {\n",
    "                    node.name: node_nlist.index(node.name)\n",
    "                }\n",
    "        # traverse the idx_list['Conv']\n",
    "        if 'Conv' not in idx_list.keys():\n",
    "            print('[ERROR MASSAGE] This graph has no operators \"Conv\" !')\n",
    "        else:\n",
    "            for idx in idx_list['Conv'].values():\n",
    "                # temp_list = utils._parse_Conv_get_pads_strides(idx, graph)\n",
    "                temp_list = []\n",
    "                # print('===========================================================')\n",
    "                # print('[DEBUG MASSAGE]')\n",
    "                # for elem in graph.node[op_idx].attribute:\n",
    "                #     print(elem)\n",
    "\n",
    "                attri_nlist = []\n",
    "                # get attribute name list\n",
    "                for elem in graph.node[idx].attribute:\n",
    "                    attri_nlist.append(elem.name)\n",
    "\n",
    "                # find pads\n",
    "                if 'pads' in attri_nlist:\n",
    "                    idx1 = attri_nlist.index('pads')\n",
    "                    temp_list.append(graph.node[idx].attribute[idx1].ints)\n",
    "                else:\n",
    "                    temp_list.append('None')\n",
    "\n",
    "                # strides\n",
    "                if 'strides' in attri_nlist:\n",
    "                    idx1 = attri_nlist.index('strides')\n",
    "                    temp_list.append(graph.node[idx].attribute[idx1].ints)\n",
    "                else:\n",
    "                    temp_list.append('None')\n",
    "                temp_tuple = (graph.node[idx].name, temp_list[0], temp_list[1])\n",
    "                print(temp_tuple)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # attr=get_info.get_Gemm_attribute(graph)\n",
    "\n",
    "        # init Gemm_attr\n",
    "        list_Gemm_attr = []\n",
    "\n",
    "        # traverse the Gemm operators and parse the attributeProtos\n",
    "        if 'Gemm' not in idx_list.keys():\n",
    "            print('[ERROR MASSAGE] This graph has no operators \"Gemm\" !')\n",
    "            return False\n",
    "        else:\n",
    "            # traverse the all the Gemm operator\n",
    "            for idx in idx_list['Gemm'].values():\n",
    "                # traverse the attributes of a Gemm operator and get the name list\n",
    "                attri_nlist = [k.name for k in graph.node[idx].attribute]\n",
    "                # get transA\n",
    "                if 'transA' in attri_nlist:\n",
    "                    attr_idx = attri_nlist.index('transA')\n",
    "                    transA = graph.node[idx].attribute[attr_idx].i\n",
    "                else:\n",
    "                    transA = 0\n",
    "                # get transB\n",
    "                if 'transB' in attri_nlist:\n",
    "                    attr_idx = attri_nlist.index('transB')\n",
    "                    transB = graph.node[idx].attribute[attr_idx].i\n",
    "                else:\n",
    "                    transB = 0\n",
    "                # get alpha\n",
    "                if 'alpha' in attri_nlist:\n",
    "                    attr_idx = attri_nlist.index('alpha')\n",
    "                    alpha = graph.node[idx].attribute[attr_idx].f\n",
    "                else:\n",
    "                    alpha = float(1.0)\n",
    "                # get beta\n",
    "                if 'beta' in attri_nlist:\n",
    "                    attr_idx = attri_nlist.index('beta')\n",
    "                    beta = graph.node[idx].attribute[attr_idx].f\n",
    "                else:\n",
    "                    beta = float(1.0)\n",
    "                # collect the information\n",
    "                temp_tuple = (graph.node[idx].name, transA, transB, alpha, beta)\n",
    "                # append to the list\n",
    "                list_Gemm_attr.append(temp_tuple)\n",
    "\n",
    "        print(list_Gemm_attr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unable to display: \" + str(e))\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "def get_valueproto_or_tensorproto_by_name(name: str, graph: xpb2.GraphProto):\n",
    "    for i, node in enumerate(inferred_model.graph.node):\n",
    "            if node.name == \"\":\n",
    "                inferred_model.graph.node[i].name = str(i)\n",
    "    input_nlist = [k.name for k in graph.input]\n",
    "    initializer_nlist = [k.name for k in graph.initializer]\n",
    "    value_info_nlist = [k.name for k in graph.value_info]\n",
    "    output_nlist = [k.name for k in graph.output]\n",
    "\n",
    "    # get tensor data\n",
    "    if name in input_nlist:\n",
    "        idx = input_nlist.index(name)\n",
    "        return graph.input[idx], int(1)\n",
    "    elif name in value_info_nlist:\n",
    "        idx = value_info_nlist.index(name)\n",
    "        return graph.value_info[idx], int(2)\n",
    "    elif name in initializer_nlist:\n",
    "        idx = initializer_nlist.index(name)\n",
    "        return graph.initializer[idx], int(3)\n",
    "    elif name in output_nlist:\n",
    "        idx = output_nlist.index(name)\n",
    "        return graph.output[idx], int(4)\n",
    "    else:\n",
    "        print(\"[ERROR MASSAGE] Can't find the tensor: \", name)\n",
    "        print('input_nlist:\\n', input_nlist)\n",
    "        print('===================')\n",
    "        print('value_info_nlist:\\n', value_info_nlist)\n",
    "        print('===================')\n",
    "        print('initializer_nlist:\\n', initializer_nlist)\n",
    "        print('===================')\n",
    "        print('output_nlist:\\n', output_nlist)\n",
    "        print('===================')\n",
    "        return False, 0\n",
    "def cal_tensor_mem_size(elem_type: str, shape: [int]):\n",
    "    \"\"\" given the element type of the tensor and its shape, and return its memory size.\n",
    "\n",
    "    Utility.\n",
    "\n",
    "    Args:\n",
    "        ttype: the type of the element of the given tensor. format: 'int', ...\n",
    "        shape: the shape of the given tensor. format: [] of int\n",
    "\n",
    "    Returns:\n",
    "        mem_size: int\n",
    "    \"\"\"\n",
    "    # init\n",
    "    mem_size = int(1)\n",
    "    # traverse the list to get the number of the elements\n",
    "    for num in shape:\n",
    "        mem_size *= num\n",
    "    # multiple the size of variable with the number of the elements\n",
    "    # \"FLOAT\": 1,\n",
    "    # \"UINT8\": 2,\n",
    "    # \"INT8\": 3,\n",
    "    # \"UINT16\": 4,\n",
    "    # \"INT16\": 5,\n",
    "    # \"INT32\": 6,\n",
    "    # \"INT64\": 7,\n",
    "    # # \"STRING\" : 8,\n",
    "    # \"BOOL\": 9,\n",
    "    # \"FLOAT16\": 10,\n",
    "    # \"DOUBLE\": 11,\n",
    "    # \"UINT32\": 12,\n",
    "    # \"UINT64\": 13,\n",
    "    # \"COMPLEX64\": 14,\n",
    "    # \"COMPLEX128\": 15\n",
    "    if elem_type == 1:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 2:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 3:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 4:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 5:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 6:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 7:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 9:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 10:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 11:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 12:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 13:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 14:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 15:\n",
    "        mem_size *= 16\n",
    "    else:\n",
    "        print(\"Undefined data type\")\n",
    "\n",
    "    return mem_size\n",
    "def get_bandwidth(graph: xpb2.GraphProto):\n",
    "    #try:\n",
    "    mem_BW_list = []\n",
    "    total_mem_BW = 0\n",
    "    unknown_tensor_list = []\n",
    "    # traverse all the nodes\n",
    "    for nodeProto in graph.node:\n",
    "        # init variables\n",
    "        read_mem_BW_each_layer = 0\n",
    "        write_mem_BW_each_layer = 0\n",
    "        total_each_layer = 0\n",
    "        # traverse all input tensor\n",
    "        for input_name in nodeProto.input:\n",
    "            # get the TensorProto/ValueInfoProto by searching its name\n",
    "            proto, type_Num = get_valueproto_or_tensorproto_by_name(\n",
    "                input_name, graph)\n",
    "            # parse the ValueInfoProto/TensorProto\n",
    "            if proto:\n",
    "                if type_Num == 3:\n",
    "                    dtype = getattr(proto, 'data_type', False)\n",
    "                    # get the shape of the tensor\n",
    "                    shape = getattr(proto, 'dims', [])\n",
    "                elif type_Num == 1 or type_Num == 2:\n",
    "                    name, dtype, shape_str = _parse_element_(proto)\n",
    "                    if(len(shape_str)>2):#改了這裡\n",
    "                        shape_str = shape_str.strip('[]')\n",
    "                        shape_str = shape_str.split(',')\n",
    "                        shape = []\n",
    "                        for dim in shape_str:\n",
    "                            shape.append(int(dim))\n",
    "                else:\n",
    "                    print(\n",
    "                        '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                        input_name, ' is from a wrong list !')\n",
    "            else:\n",
    "                print(\n",
    "                    '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                    input_name, ' is no found !')\n",
    "                unknown_tensor_list.append(\n",
    "                    (nodeProto.name, input_name, nodeProto.op_type))\n",
    "            # calculate the tensor size in btye\n",
    "            \n",
    "            read_mem_BW_each_layer += cal_tensor_mem_size(dtype, shape)\n",
    "\n",
    "        # traverse all output tensor\n",
    "        for output_name in nodeProto.output:\n",
    "            # get the TensorProto/ValueInfoProto by searching its name\n",
    "            proto, type_Num = get_valueproto_or_tensorproto_by_name(\n",
    "                output_name, graph)\n",
    "            # parse the ValueInfoProto\n",
    "            if proto:\n",
    "                if type_Num == 2 or type_Num == 4:\n",
    "                    # name, dtype, shape = utils._parse_ValueInfoProto(proto)\n",
    "                    name, dtype, shape_str = _parse_element_(proto)\n",
    "                    \n",
    "                    if(len(shape_str)>2):#改了這裡\n",
    "                        shape_str = shape_str.strip('[]')\n",
    "                        shape_str = shape_str.split(',')\n",
    "                        shape = []\n",
    "                        for dim in shape_str:\n",
    "                            shape.append(int(dim))\n",
    "                else:\n",
    "                    print(\n",
    "                        '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                        output_name, ' is from a wrong list !')\n",
    "            else:\n",
    "                print(\n",
    "                    '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                    input_name, ' is no found !')\n",
    "                unknown_tensor_list.append(\n",
    "                    (nodeProto.name, output_name, nodeProto.op_type))\n",
    "            # calculate the tensor size in btye\n",
    "            write_mem_BW_each_layer += cal_tensor_mem_size(dtype, shape)\n",
    "\n",
    "        # cal total bw\n",
    "        total_each_layer = read_mem_BW_each_layer + write_mem_BW_each_layer\n",
    "\n",
    "        # store into tuple\n",
    "        temp_tuple = (nodeProto.name, read_mem_BW_each_layer,\n",
    "                    write_mem_BW_each_layer, total_each_layer)\n",
    "        #append it\n",
    "        mem_BW_list.append(temp_tuple)\n",
    "        # accmulate the value\n",
    "        total_mem_BW += total_each_layer\n",
    "\n",
    "    # display the mem_bw of eahc layer\n",
    "    columns = ['layer', 'read_bw', 'write_bw', 'total_bw']\n",
    "    # resort the list\n",
    "    mem_BW_list = sorted(mem_BW_list,\n",
    "                            key=lambda Layer: Layer[1],\n",
    "                            reverse=True)\n",
    "    #print(tabulate(mem_BW_list, headers=columns))\n",
    "    print(\n",
    "        '====================================================================================\\n'\n",
    "    )\n",
    "    # display it\n",
    "    print(\"2-2-2. Data bandwidth requirement\\n\")\n",
    "    \n",
    "    print(\n",
    "        \"The memory bandwidth for processor to execute a whole model without on-chip-buffer is: \\n\",\n",
    "        total_mem_BW, '(bytes)\\n',\n",
    "        float(total_mem_BW) / float(1000000), '(MB)\\n')\n",
    "    # display the unknown tensor\n",
    "    columns = ['op_name', 'unfound_tensor', 'op_type']\n",
    "    #print(tabulate(unknown_tensor_list, headers=columns))\n",
    "    print(\n",
    "        '====================================================================================\\n'\n",
    "    )\n",
    "    # except Exception as e:\n",
    "    #     print(\"[ERROR MASSAGE] Unable to display: \" + str(e))\n",
    "    #     return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6890b5b1-d99d-4574-a043-44bb2b674f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tensor_size(graph: xpb2.GraphProto):\n",
    "    # main.List_Tensor_SizeOfConv(graph)\n",
    "    try:\n",
    "        for i, node in enumerate(inferred_model.graph.node):\n",
    "                if node.name == \"\":\n",
    "                    inferred_model.graph.node[i].name = str(i)\n",
    "        # get the list\n",
    "        All_Conv_tensor_size = []\n",
    "        All_Gemm_tensor_size = []\n",
    "        bias = False\n",
    "        # get the idx of the operators\n",
    "        if type(graph) is not xpb2.GraphProto:\n",
    "            sys.exit('The input graph is not a GraphProto!')\n",
    "\n",
    "        node_nList = [k.name for k in graph.node]\n",
    "        input_nlist = [k.name for k in graph.input]\n",
    "        initializer_nlist = [k.name for k in graph.initializer]\n",
    "        value_info_nlist = [k.name for k in graph.value_info]\n",
    "        output_nlist = [k.name for k in graph.output]\n",
    "        idx_list = {}\n",
    "        for node in graph.node:\n",
    "            if node.op_type in idx_list:\n",
    "                idx_list[node.op_type][node.name] = node_nList.index(node.name)\n",
    "            else:\n",
    "                idx_list[node.op_type] = {\n",
    "                    node.name: node_nList.index(node.name)\n",
    "                }\n",
    "\n",
    "        # get the Conv tensor size\n",
    "        if 'Conv' not in idx_list.keys():\n",
    "            print('This graph has no operators \"Conv\" !')\n",
    "        else:\n",
    "            for idx in idx_list['Conv'].values():\n",
    "                # temp_tuple, bias = utils._Cal_tensor_size_ConvOrGemm(idx, graph)\n",
    "                num_conv_input_tensor = len(graph.node[idx].input)\n",
    "                list_of_data_num = []\n",
    "                # get input tensor proto\n",
    "                for input_name in graph.node[idx].input:\n",
    "                    # get tensor data\n",
    "                    if input_name in input_nlist:\n",
    "                        name_idx = input_nlist.index(input_name)\n",
    "                        data = graph.input[name_idx]\n",
    "                        type_num = int(1)\n",
    "                    elif input_name in value_info_nlist:\n",
    "                        name_idx = value_info_nlist.index(input_name)\n",
    "                        data = graph.value_info[name_idx]\n",
    "                        type_num = int(2)\n",
    "                    elif input_name in initializer_nlist:\n",
    "                        name_idx = initializer_nlist.index(input_name)\n",
    "                        data = graph.initializer[name_idx]\n",
    "                        type_num = int(3)\n",
    "                    elif input_name in output_nlist:\n",
    "                        name_idx = output_nlist.index(input_name)\n",
    "                        data = graph.output[name_idx]\n",
    "                        type_num = int(4)\n",
    "                    else:\n",
    "                        print(\"Can't find the tensor: \", input_name)\n",
    "                        print('input_nlist:\\n', input_nlist)\n",
    "                        print('===================')\n",
    "                        print('value_info_nlist:\\n', value_info_nlist)\n",
    "                        print('===================')\n",
    "                        print('initializer_nlist:\\n', initializer_nlist)\n",
    "                        print('===================')\n",
    "                        print('output_nlist:\\n', output_nlist)\n",
    "                        print('===================')\n",
    "                \n",
    "                    list_of_data_num.append((data, type_num))\n",
    "\n",
    "                if graph.node[idx].output[0] in input_nlist:\n",
    "                    name_idx = input_nlist.index(graph.node[idx].output[0])\n",
    "                    data = graph.input[name_idx]\n",
    "                    type_num = int(1)\n",
    "                elif graph.node[idx].output[0] in value_info_nlist:\n",
    "                    name_idx = value_info_nlist.index(graph.node[idx].output[0])\n",
    "                    data = graph.value_info[name_idx]\n",
    "                    type_num = int(2)\n",
    "                elif graph.node[idx].output[0] in initializer_nlist:\n",
    "                    name_idx = initializer_nlist.index(graph.node[idx].output[0])\n",
    "                    data = graph.initializer[name_idx]\n",
    "                    type_num = int(3)\n",
    "                elif graph.node[idx].output[0] in output_nlist:\n",
    "                    name_idx = output_nlist.index(graph.node[idx].output[0])\n",
    "                    data = graph.output[name_idx]\n",
    "                    type_num = int(4)\n",
    "                else:\n",
    "                    print(\"Can't find the tensor: \", graph.node[idx].output[0])\n",
    "                    print('input_nlist:\\n', input_nlist)\n",
    "                    print('===================')\n",
    "                    print('value_info_nlist:\\n', value_info_nlist)\n",
    "                    print('===================')\n",
    "                    print('initializer_nlist:\\n', initializer_nlist)\n",
    "                    print('===================')\n",
    "                    print('output_nlist:\\n', output_nlist)\n",
    "                    print('===================')\n",
    "                list_of_data_num.append((data, type_num))\n",
    "\n",
    "                list_temp = [\n",
    "                    graph.node[idx].name,\n",
    "                ]\n",
    "                for elem in list_of_data_num:\n",
    "                    if elem[0]:\n",
    "                        if elem[1] == 3:\n",
    "                            name = getattr(elem[0], 'name', \"None\")\n",
    "                            # get the data type of the tensor\n",
    "                            data_type = getattr(elem[0], 'data_type', False)\n",
    "                            # get the shape of the tensor\n",
    "                            shape = getattr(elem[0], 'dims', [])\n",
    "                        else:\n",
    "                            # name, data_type, shape = utils._parse_ValueInfoProto(elem[0])\n",
    "                            name, data_type, shape_str = _parse_element_(elem[0])\n",
    "                            if(len(shape_str)>2):\n",
    "                                shape_str = shape_str.strip('[]')\n",
    "                                shape_str = shape_str.split(',')\n",
    "                                shape = []\n",
    "                                for dim in shape_str:\n",
    "                                    shape.append(int(dim))\n",
    "                        mem_size = int(1)\n",
    "                        # traverse the list to get the number of the elements\n",
    "                        for num in shape:\n",
    "                            mem_size *= num\n",
    "                        # multiple the size of variable with the number of the elements\n",
    "                        # \"FLOAT\": 1,\n",
    "                        # \"UINT8\": 2,\n",
    "                        # \"INT8\": 3,\n",
    "                        # \"UINT16\": 4,\n",
    "                        # \"INT16\": 5,\n",
    "                        # \"INT32\": 6,\n",
    "                        # \"INT64\": 7,\n",
    "                        # # \"STRING\" : 8,\n",
    "                        # \"BOOL\": 9,\n",
    "                        # \"FLOAT16\": 10,\n",
    "                        # \"DOUBLE\": 11,\n",
    "                        # \"UINT32\": 12,\n",
    "                        # \"UINT64\": 13,\n",
    "                        # \"COMPLEX64\": 14,\n",
    "                        # \"COMPLEX128\": 15\n",
    "                        if data_type == 1:\n",
    "                            mem_size *= 4\n",
    "                        elif data_type == 2:\n",
    "                            mem_size *= 1\n",
    "                        elif data_type == 3:\n",
    "                            mem_size *= 1\n",
    "                        elif data_type == 4:\n",
    "                            mem_size *= 2\n",
    "                        elif data_type == 5:\n",
    "                            mem_size *= 2\n",
    "                        elif data_type == 6:\n",
    "                            mem_size *= 4\n",
    "                        elif data_type == 7:\n",
    "                            mem_size *= 8\n",
    "                        elif data_type == 9:\n",
    "                            mem_size *= 1\n",
    "                        elif data_type == 10:\n",
    "                            mem_size *= 2\n",
    "                        elif data_type == 11:\n",
    "                            mem_size *= 8\n",
    "                        elif data_type == 12:\n",
    "                            mem_size *= 4\n",
    "                        elif data_type == 13:\n",
    "                            mem_size *= 8\n",
    "                        elif data_type == 14:\n",
    "                            mem_size *= 8\n",
    "                        elif data_type == 15:\n",
    "                            mem_size *= 16\n",
    "                        list_temp.append(mem_size)\n",
    "                    else:\n",
    "                        print(graph.node[idx].name, 'tenosr no found ! Something wrong')\n",
    "\n",
    "                if len(list_of_data_num) > 3:  # the conv has bias\n",
    "                    ConvOrGemm_tensor_size = (list_temp[0], list_temp[1], list_temp[2],\n",
    "                                            list_temp[3], list_temp[4], list_temp[1] +\n",
    "                                            list_temp[2] + list_temp[3] + list_temp[4])\n",
    "                    bias = True\n",
    "                else:  # the conv has no bias\n",
    "                    ConvOrGemm_tensor_size = (list_temp[0], list_temp[1], list_temp[2],\n",
    "                                            list_temp[3],\n",
    "                                            list_temp[1] + list_temp[2] + list_temp[3])\n",
    "                    bias = False\n",
    "\n",
    "                All_Conv_tensor_size.append(ConvOrGemm_tensor_size)\n",
    "    # main.List_Tensor_SizeOfGemm(graph)\n",
    "        if 'Gemm' not in idx_list.keys():\n",
    "            print('This graph has no operators \"Gemm\" !')\n",
    "        else:\n",
    "            for idx in idx_list['Gemm'].values():\n",
    "                num_conv_input_tensor = len(graph.node[idx].input)\n",
    "                list_of_data_num = []\n",
    "                for input_name in graph.node[idx].input:\n",
    "                    if input_name in input_nlist:\n",
    "                        name_idx = input_nlist.index(input_name)\n",
    "                        data = graph.input[name_idx]\n",
    "                        type_num = int(1)\n",
    "                    elif input_name in value_info_nlist:\n",
    "                        name_idx = value_info_nlist.index(input_name)\n",
    "                        data = graph.value_info[name_idx]\n",
    "                        type_num = int(2)\n",
    "                    elif input_name in initializer_nlist:\n",
    "                        name_idx = initializer_nlist.index(input_name)\n",
    "                        data = graph.initializer[name_idx]\n",
    "                        type_num = int(3)\n",
    "                    elif input_name in output_nlist:\n",
    "                        name_idx = output_nlist.index(input_name)\n",
    "                        data = graph.output[name_idx]\n",
    "                        type_num = int(4)\n",
    "                    else:\n",
    "                        print(\"Can't find the tensor: \", input_name)\n",
    "                        print('input_nlist:\\n', input_nlist)\n",
    "                        print('===================')\n",
    "                        print('value_info_nlist:\\n', value_info_nlist)\n",
    "                        print('===================')\n",
    "                        print('initializer_nlist:\\n', initializer_nlist)\n",
    "                        print('===================')\n",
    "                        print('output_nlist:\\n', output_nlist)\n",
    "                        print('===================')\n",
    "                \n",
    "                    list_of_data_num.append((data, type_num))\n",
    "                if graph.node[idx].output[0] in input_nlist:\n",
    "                    name_idx = input_nlist.index(graph.node[idx].output[0])\n",
    "                    data = graph.input[name_idx]\n",
    "                    type_num = int(1)\n",
    "                elif graph.node[idx].output[0] in value_info_nlist:\n",
    "                    name_idx = value_info_nlist.index(graph.node[idx].output[0])\n",
    "                    data = graph.value_info[name_idx]\n",
    "                    type_num = int(2)\n",
    "                elif graph.node[idx].output[0] in initializer_nlist:\n",
    "                    name_idx = initializer_nlist.index(graph.node[idx].output[0])\n",
    "                    data = graph.initializer[name_idx]\n",
    "                    type_num = int(3)\n",
    "                elif graph.node[idx].output[0] in output_nlist:\n",
    "                    name_idx = output_nlist.index(graph.node[idx].output[0])\n",
    "                    data = graph.output[name_idx]\n",
    "                    type_num = int(4)\n",
    "                else:\n",
    "                    print(\"Can't find the tensor: \", graph.node[idx].output[0])\n",
    "                    print('input_nlist:\\n', input_nlist)\n",
    "                    print('===================')\n",
    "                    print('value_info_nlist:\\n', value_info_nlist)\n",
    "                    print('===================')\n",
    "                    print('initializer_nlist:\\n', initializer_nlist)\n",
    "                    print('===================')\n",
    "                    print('output_nlist:\\n', output_nlist)\n",
    "                    print('===================')\n",
    "                list_of_data_num.append((data, type_num))\n",
    "                list_temp = [\n",
    "                    graph.node[idx].name,\n",
    "                ]\n",
    "                for elem in list_of_data_num:\n",
    "                    if elem[0]:\n",
    "                        if elem[1] == 3:\n",
    "                            name = getattr(elem[0], 'name', \"None\")\n",
    "                            data_type = getattr(elem[0], 'data_type', False)\n",
    "                            shape = getattr(elem[0], 'dims', [])\n",
    "                        else:\n",
    "                            name, data_type, shape_str = _parse_element_(elem[0])\n",
    "                            if(len(shape_str)>2):\n",
    "                                shape_str = shape_str.strip('[]')\n",
    "                                shape_str = shape_str.split(',')\n",
    "                                shape = []\n",
    "                                for dim in shape_str:\n",
    "                                    shape.append(int(dim))\n",
    "                        mem_size = int(1)\n",
    "                        # traverse the list to get the number of the elements\n",
    "                        for num in shape:\n",
    "                            mem_size *= num\n",
    "                        # multiple the size of variable with the number of the elements\n",
    "                        if data_type == 'FLOAT':\n",
    "                            mem_size *= 8\n",
    "                        elif data_type == 'UINT8':\n",
    "                            mem_size *= 1\n",
    "                        elif data_type == 'INT8':\n",
    "                            mem_size *= 1\n",
    "                        elif data_type == 'UINT16':\n",
    "                            mem_size *= 2\n",
    "                        elif data_type == 'INT16':\n",
    "                            mem_size *= 2\n",
    "                        elif data_type == 'INT32':\n",
    "                            mem_size *= 4\n",
    "                        elif data_type == 'INT64':\n",
    "                            mem_size *= 8\n",
    "                        elif data_type == 'BOOL':\n",
    "                            mem_size *= 1\n",
    "                        elif data_type == 'FLOAT16':\n",
    "                            mem_size *= 2\n",
    "                        elif data_type == 'DOUBLE':\n",
    "                            mem_size *= 8\n",
    "                        elif data_type == 'UINT32':\n",
    "                            mem_size *= 4\n",
    "                        elif data_type == 'UINT64':\n",
    "                            mem_size *= 8\n",
    "                        elif data_type == 'COMPLEX64':\n",
    "                            mem_size *= 8\n",
    "                        elif data_type == 'COMPLEX128':\n",
    "                            mem_size *= 16\n",
    "                        else:\n",
    "                            print(\"Undefined data type\")\n",
    "                        list_temp.append(mem_size)\n",
    "                    else:\n",
    "                        print(graph.node[idx].name, 'tenosr no found ! Something wrong')\n",
    "\n",
    "                if len(list_of_data_num) > 3:  # the conv has bias\n",
    "                    ConvOrGemm_tensor_size = (list_temp[0], list_temp[1], list_temp[2],\n",
    "                                            list_temp[3], list_temp[4], list_temp[1] +\n",
    "                                            list_temp[2] + list_temp[3] + list_temp[4])\n",
    "                    bias = True\n",
    "                else:  # the conv has no bias\n",
    "                    ConvOrGemm_tensor_size = (list_temp[0], list_temp[1], list_temp[2],\n",
    "                                            list_temp[3],\n",
    "                                            list_temp[1] + list_temp[2] + list_temp[3])\n",
    "                    bias = False\n",
    "\n",
    "                All_Gemm_tensor_size.append(ConvOrGemm_tensor_size)  \n",
    "        # print\n",
    "        print(\"2-2-3. activation memory storage requirement\\n\")\n",
    "        if 'Conv' not in idx_list.keys():\n",
    "            print('returning from List_Tensor_SizeOfConv() ...')\n",
    "        else:\n",
    "            if bias:\n",
    "                columns = [\n",
    "                    'Conv_name', 'Input_tesnor', 'Weight_tensor',\n",
    "                    'Bias_tensor', 'Output_tensor', 'Total'\n",
    "                ]\n",
    "            else:\n",
    "                columns = [\n",
    "                    'Conv_name', 'Input_tesnor', 'Weight_tensor',\n",
    "                    'Output_tensor', 'Total'\n",
    "                ]\n",
    "\n",
    "            # resort the list\n",
    "            All_Conv_tensor_size = sorted(All_Conv_tensor_size,\n",
    "                                          key=lambda Layer: Layer[-1],\n",
    "                                          reverse=True)\n",
    "            print('list_of_layer RESORTED ! ')\n",
    "            print(tabulate(All_Conv_tensor_size, headers=columns))\n",
    "            print(\n",
    "                '====================================================================================\\n'\n",
    "            )\n",
    "\n",
    "        if 'Gemm' not in idx_list.keys():\n",
    "            print('returning from List_Tensor_SizeOfGemm() ...')\n",
    "        else:\n",
    "            if bias:\n",
    "                columns = [\n",
    "                    'Gemm_name', 'Mat_A', 'Mat_B', 'Mat_C', 'Mat_Y', 'Total'\n",
    "                ]\n",
    "            else:\n",
    "                columns = ['Gemm_name', 'Mat_A', 'Mat_B', 'Mat_Y', 'Total']\n",
    "\n",
    "            # resort the list\n",
    "            All_Gemm_tensor_size = sorted(All_Gemm_tensor_size,\n",
    "                                          key=lambda Layer: Layer[-1],\n",
    "                                          reverse=True)\n",
    "            print('list_of_layer RESORTED ! ')\n",
    "            print(tabulate(All_Gemm_tensor_size, headers=columns))\n",
    "            print(\n",
    "                '====================================================================================\\n'\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unable to display: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13fca2a-e38a-427c-bb9f-36b7e4d6aca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "====================================================================================\n",
      "\n",
      "2-2-2. Data bandwidth requirement\n",
      "\n",
      "The memory bandwidth for processor to execute a whole model without on-chip-buffer is: \n",
      " 13951528 (bytes)\n",
      " 13.951528 (MB)\n",
      "\n",
      "====================================================================================\n",
      "\n",
      "Undefined data type\n",
      "Undefined data type\n",
      "Undefined data type\n",
      "Undefined data type\n",
      "2-2-3. activation memory storage requirement\n",
      "\n",
      "list_of_layer RESORTED ! \n",
      "Conv_name      Input_tesnor    Weight_tensor    Bias_tensor    Output_tensor    Total\n",
      "-----------  --------------  ---------------  -------------  ---------------  -------\n",
      "Conv_95                   0          1638400           5120                0  1643520\n",
      "Conv_94                   0          1228800           1280                0  1230080\n",
      "Conv_78                   0           614400           3840                0   618240\n",
      "Conv_84                   0           614400           3840                0   618240\n",
      "Conv_90                   0           614400           3840                0   618240\n",
      "Conv_82                   0           614400            640                0   615040\n",
      "Conv_88                   0           614400            640                0   615040\n",
      "Conv_77                   0           368640            640                0   369280\n",
      "Conv_61                   0           221184           2304                0   223488\n",
      "Conv_67                   0           221184           2304                0   223488\n",
      "Conv_73                   0           221184           2304                0   223488\n",
      "Conv_65                   0           221184            384                0   221568\n",
      "Conv_71                   0           221184            384                0   221568\n",
      "Conv_60                   0           147456            384                0   147840\n",
      "Conv_38                   0            98304           1536                0    99840\n",
      "Conv_44                   0            98304           1536                0    99840\n",
      "Conv_50                   0            98304           1536                0    99840\n",
      "Conv_56                   0            98304           1536                0    99840\n",
      "Conv_42                   0            98304            256                0    98560\n",
      "Conv_48                   0            98304            256                0    98560\n",
      "Conv_54                   0            98304            256                0    98560\n",
      "Conv_37                   0            49152            256                0    49408\n",
      "Conv_80                   0            34560           3840                0    38400\n",
      "Conv_86                   0            34560           3840                0    38400\n",
      "Conv_92                   0            34560           3840                0    38400\n",
      "Conv_21                   0            24576            768                0    25344\n",
      "Conv_27                   0            24576            768                0    25344\n",
      "Conv_33                   0            24576            768                0    25344\n",
      "Conv_25                   0            24576            128                0    24704\n",
      "Conv_31                   0            24576            128                0    24704\n",
      "Conv_63                   0            20736           2304                0    23040\n",
      "Conv_69                   0            20736           2304                0    23040\n",
      "Conv_75                   0            20736           2304                0    23040\n",
      "Conv_20                   0            18432            128                0    18560\n",
      "Conv_40                   0            13824           1536                0    15360\n",
      "Conv_46                   0            13824           1536                0    15360\n",
      "Conv_52                   0            13824           1536                0    15360\n",
      "Conv_58                   0            13824           1536                0    15360\n",
      "Conv_10                   0            13824            576                0    14400\n",
      "Conv_16                   0            13824            576                0    14400\n",
      "Conv_14                   0            13824             96                0    13920\n",
      "Conv_9                    0             9216             96                0     9312\n",
      "Conv_23                   0             6912            768                0     7680\n",
      "Conv_29                   0             6912            768                0     7680\n",
      "Conv_35                   0             6912            768                0     7680\n",
      "Conv_5                    0             6144            384                0     6528\n",
      "Conv_12                   0             5184            576                0     5760\n",
      "Conv_18                   0             5184            576                0     5760\n",
      "Conv_7                    0             3456            384                0     3840\n",
      "Conv_0                    0             3456            128                0     3584\n",
      "Conv_4                    0             2048             64                0     2112\n",
      "Conv_2                    0             1152            128                0     1280\n",
      "====================================================================================\n",
      "\n",
      "list_of_layer RESORTED ! \n",
      "Gemm_name      Mat_A    Mat_B    Mat_C    Mat_Y    Total\n",
      "-----------  -------  -------  -------  -------  -------\n",
      "Gemm_104           0  1280000     1000        0  1281000\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#從這裡開始\n",
    "print(\"start\")\n",
    "# find_operator(inferred_model.graph)#ok\n",
    "# get_attribute(inferred_model.graph)#ok\n",
    "get_bandwidth(inferred_model.graph)\n",
    "list_tensor_size(inferred_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479800c-a9bc-47fd-ad10-06cb21b4bae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70378cb5-563c-4118-9d01-cfef61d1effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total activation memory: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454fdaf-4c12-425b-93c2-9c09d9c29cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
